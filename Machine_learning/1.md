## 基础
- 监督学习（Supervised Learning）：a datasat with labels
  - 回归问题（Regression）：连续
  - 分类问题（Classification）：离散
- 无监督学习（Unsupervised Learning）：a dataset with no labels
  - 聚类算法（Clustering Algorithms）
  - 降维算法（Dimensionality Reduction Algorithm）
- 符号
  -  m：样本数量
  -  x：输入变量/特征
  -  y：输出变量/预测的目标变量
  -  （x，y）：一个训练样本
  -  h：假设函数
## 概念
- 一元线性回归（单变量线性回归）
  - 代价函数（平方误差函数/损失函数）：二分之一（方差和的平均值） （/m是为了使值不那么大，多个2是为了方便求导）
    - ![代价函数公式](image-1.png) 
  - 梯度下降法：sovle more general problems
    - 学习率：梯度下降时迈出多大步子  
    - 两个参数要同步更新
    - ![梯度下降公式](image.png)
    - 越靠近局部最优解下降越慢
  - 线性回归中的梯度下降--Batch Gradient Descent
    - ![Batch梯度下降](image-2.png)
  - 矩阵
    - 只有方阵才有逆矩阵，且行列式不为0
- 多元线性回归
  - ![ ](image-4.png)
  - ![ ](image-5.png)
  - 特征放缩（因为特征值的取值范围差别较大 ）
    - 归一化来使梯度下降更快
    - 标准化（均值归一化）：使平均值等于0
      - ![Alt text](image-3.png)
      - ![Alt text](image-6.png)
  - 学习率 一次迭代是指走一步下山还是一次下山，而且这样子得到的不是局部最优解吗，起点在哪里
- 正规方程（偏导数=0的矩阵写法）
  - 最优解：![Alt text](image-7.png)（最小二乘法？）
  - 用正规方程法则不需要进行特征放缩
- 梯度下降法和正规矩阵法
  - ![Alt text](image-8.png) 
- logistic回归算法（sigmoid function）
  - 一种分类算法
  - 算法输出介于0和1之间
  - ![Alt text](image-9.png)
  - /34